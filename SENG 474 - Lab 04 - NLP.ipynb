{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Words to Add to Bag from Dataset\n",
    "\n",
    "data from: https://www.kaggle.com/uciml/sms-spam-collection-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = [\"spam\", \"ham\"]\n",
    "\n",
    "def load_spam_data():\n",
    "\n",
    "    # Data File\n",
    "    file = \"data/spam.csv\"\n",
    "\n",
    "    # Lists to store all word frequencies from messages\n",
    "    # used to find top words for each class\n",
    "    content_spam = {}\n",
    "    content_ham = {}\n",
    "\n",
    "    # Lists for each message and corresponding label\n",
    "    messages = []\n",
    "    y = []\n",
    "\n",
    "    # open the file for processing as a CSV\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        for i, row in enumerate(reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            # split the string and remove all non alpha characters (or ')\n",
    "            words = [''.join(c for c in word if c.isalpha() or c == \"'\") for word in row[1].lower().split()]\n",
    "\n",
    "            # Add and count words for spam and ham classes\n",
    "            content = content_spam if row[0] == \"spam\" else content_ham\n",
    "            for w in words:\n",
    "                if len(w) > 3:\n",
    "                    if w in content:\n",
    "                        content[w] += 1\n",
    "                    else:\n",
    "                        content[w] = 1\n",
    "\n",
    "            # Append full messages\n",
    "            messages.append(\" \".join(words))\n",
    "            y.append(classes.index(row[0]))\n",
    "\n",
    "    # sort each each word based on value count\n",
    "    sorted_X_spam = sorted(content_spam, key=content_spam.get, reverse=True)\n",
    "    sorted_X_ham = sorted(content_ham, key=content_ham.get, reverse=True)\n",
    "\n",
    "    # populate the bag-of-words with top 50 words from each class (and remove duplicates)\n",
    "    bow = []\n",
    "    for i in range(50):\n",
    "        if sorted_X_spam[i] not in bow:\n",
    "            bow.append(sorted_X_spam[i])\n",
    "        if sorted_X_ham[i] not in bow:\n",
    "            bow.append(sorted_X_ham[i])\n",
    "\n",
    "\n",
    "    return bow, messages, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow, messages, y = load_spam_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Messages into Word Frequency Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = CountVectorizer(vocabulary=bow).fit_transform(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had your mobile  months or more u r entitled to update to the latest colour mobiles with camera for free call the mobile update co free on \n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 8)\t2\n",
      "  (0, 13)\t1\n",
      "  (0, 57)\t1\n",
      "  (0, 62)\t1\n",
      "  (0, 76)\t1\n"
     ]
    }
   ],
   "source": [
    "i = 9\n",
    "print (messages[i])\n",
    "print (X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Classifier\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Probability based classification using Bayes Theorem\n",
    "\n",
    "* Bayes Theorem\n",
    "    * `P(Y|X) = P(X|Y) * P(Y) / P(X)`\n",
    "    * `posterior = prior * likelihood / evidence`\n",
    "    \n",
    "    \n",
    "* Naive Bayes\n",
    "    * Assumes conditional independence between features\n",
    "    * Even though this isn't always the case...\n",
    "    \n",
    "    \n",
    "* Conditional Independence:\n",
    "    * If X, Y, Z are variables, and P(X|Y, Z) = P(X|Y)\n",
    "    * Then Y and Z are conditionally independent since Z didn't affect the probability.\n",
    "    \n",
    "    \n",
    "* Bayesian Belief Networks\n",
    "    * Use when strong depedence between states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\[c\\ =\\ argmax_{x \\in \\{1,...,K\\}}\\;  p(C_k) \\prod_{i=1}^{n} p(x_i\\ |\\ C_k)\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\[c\\ =\\ argmax_{x \\in \\{1,...,K\\}}\\;  p(C_k) \\prod_{i=1}^{n} p(x_i\\ |\\ C_k)\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95692749461593685"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 0\n"
     ]
    }
   ],
   "source": [
    "p = clf.predict(X[9])\n",
    "print (p, y[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94348762  0.95261174  0.95961228]\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(clf, X, y, cv=3)\n",
    "print (score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
